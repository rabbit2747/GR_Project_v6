# Monitoring (ëª¨ë‹ˆí„°ë§)

## ğŸ“‹ êµ¬ì„±ìš”ì†Œ ì •ë³´

| ì†ì„± | ê°’ |
|------|-----|
| **êµ¬ì„±ìš”ì†Œëª…** | Monitoring |
| **í•œê¸€ëª…** | ëª¨ë‹ˆí„°ë§ |
| **Layer** | Cross-Layer (Management) |
| **ë¶„ë¥˜** | Observability |
| **Function Tag (Primary)** | M1.1 (Metrics) |
| **Function Tag (Secondary)** | M1.2 (Logging), M1.3 (Tracing) |
| **Function Tag (Control)** | ì—†ìŒ |

---

## ğŸ¯ ì •ì˜

Monitoringì€ **ì‹œìŠ¤í…œì˜ ìƒíƒœë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘, ë¶„ì„, ì‹œê°í™”í•˜ëŠ” ê´€ì°°ì„±(Observability) ì‹œìŠ¤í…œ**ì…ë‹ˆë‹¤.

---

## ğŸ—ï¸ 3ê°€ì§€ Observability Pillar

### 1. Metrics (ë©”íŠ¸ë¦­)

**Prometheus + Grafana**

```yaml
# Prometheus ì„¤ì •
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'node'
    static_configs:
      - targets: ['localhost:9100']

  - job_name: 'application'
    static_configs:
      - targets: ['app1:8080', 'app2:8080']
```

```python
# Python ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
from prometheus_client import Counter, Histogram, start_http_server
import time

# ë©”íŠ¸ë¦­ ì •ì˜
request_count = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint'])
request_duration = Histogram('http_request_duration_seconds', 'HTTP request latency')

@request_duration.time()
def handle_request(method, endpoint):
    request_count.labels(method=method, endpoint=endpoint).inc()
    # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
    time.sleep(0.1)

# Prometheus ì„œë²„ ì‹œì‘
start_http_server(8000)
```

**ì£¼ìš” ë©”íŠ¸ë¦­**:
```yaml
ì¸í”„ë¼:
  - CPU ì‚¬ìš©ë¥ : node_cpu_seconds_total
  - ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : node_memory_MemAvailable_bytes
  - ë””ìŠ¤í¬ ì‚¬ìš©ë¥ : node_filesystem_avail_bytes

ì• í”Œë¦¬ì¼€ì´ì…˜:
  - ìš”ì²­ ìˆ˜: http_requests_total
  - ì‘ë‹µ ì‹œê°„: http_request_duration_seconds
  - ì—ëŸ¬ìœ¨: http_requests_failed_total

ë°ì´í„°ë² ì´ìŠ¤:
  - ì—°ê²° ìˆ˜: db_connections_active
  - ì¿¼ë¦¬ ì‹œê°„: db_query_duration_seconds
  - íŠ¸ëœì­ì…˜ ìˆ˜: db_transactions_total
```

---

### 2. Logging (ë¡œê¹…)

**ELK Stack (Elasticsearch + Logstash + Kibana)**

```yaml
# Filebeat ì„¤ì •
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/log/nginx/*.log
      - /var/log/app/*.log

output.elasticsearch:
  hosts: ["localhost:9200"]
  index: "logs-%{+yyyy.MM.dd}"
```

```python
# êµ¬ì¡°í™”ëœ ë¡œê¹…
import logging
import json

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            'timestamp': self.formatTime(record),
            'level': record.levelname,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        }
        return json.dumps(log_data)

logger = logging.getLogger(__name__)
handler = logging.StreamHandler()
handler.setFormatter(JSONFormatter())
logger.addHandler(handler)

# ì‚¬ìš©
logger.info('User login successful', extra={'user_id': 123, 'ip': '192.168.1.1'})
```

---

### 3. Tracing (ë¶„ì‚° ì¶”ì )

**Jaeger (OpenTelemetry)**

```python
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# Tracer ì„¤ì •
trace.set_tracer_provider(TracerProvider())
jaeger_exporter = JaegerExporter(
    agent_host_name="localhost",
    agent_port=6831,
)
trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(jaeger_exporter)
)

tracer = trace.get_tracer(__name__)

# Span ìƒì„±
def process_order(order_id):
    with tracer.start_as_current_span("process_order") as span:
        span.set_attribute("order.id", order_id)

        # í•˜ìœ„ ì‘ì—…
        with tracer.start_as_current_span("validate_order"):
            validate(order_id)

        with tracer.start_as_current_span("charge_payment"):
            charge(order_id)

        with tracer.start_as_current_span("ship_order"):
            ship(order_id)
```

---

## ğŸ“Š AWS CloudWatch

```python
import boto3

cloudwatch = boto3.client('cloudwatch')

# ë©”íŠ¸ë¦­ ì „ì†¡
cloudwatch.put_metric_data(
    Namespace='MyApp',
    MetricData=[
        {
            'MetricName': 'ResponseTime',
            'Value': 0.5,
            'Unit': 'Seconds',
            'Timestamp': datetime.utcnow()
        },
    ]
)

# ì•ŒëŒ ìƒì„±
cloudwatch.put_metric_alarm(
    AlarmName='HighCPU',
    ComparisonOperator='GreaterThanThreshold',
    EvaluationPeriods=2,
    MetricName='CPUUtilization',
    Namespace='AWS/EC2',
    Period=300,
    Statistic='Average',
    Threshold=80.0,
    ActionsEnabled=True,
    AlarmActions=['arn:aws:sns:region:account-id:topic-name']
)
```

**ê°€ê²©**:
```yaml
CloudWatch:
  - Metrics: $0.30 per metric/ì›”
  - Logs: $0.50 per GB ingested
  - Alarms: $0.10 per alarm/ì›”

Datadog:
  - Pro: $15 per host/ì›”
  - Enterprise: $23 per host/ì›”
```

---

## ğŸš¨ ì•ŒëŒ ì„¤ì •

```yaml
# Prometheus Alertmanager
groups:
  - name: example
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_failed_total[5m]) > 0.05
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }}% over the last 5 minutes"

      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
```

---

## ğŸ”’ Zoneë³„ ë°°ì¹˜

| Zone | ë°°ì¹˜ | ìš©ë„ |
|------|------|------|
| **Zone 0** | Very Common | ì¤‘ì•™ ëª¨ë‹ˆí„°ë§ ì„œë²„ |
| **All Zones** | Very Common | ì—ì´ì „íŠ¸ ë°°í¬ |

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- [Cross-Layer ì •ì˜](../00_CrossLayer_ì •ì˜.md)
- [SIEM](../04_SIEM/00_SIEM_ì •ì˜.md)

---

**ë¬¸ì„œ ë**
